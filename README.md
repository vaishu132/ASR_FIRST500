---

# ğŸ™ï¸ NeMo ASR FastAPI Server

Deploy a production-ready Automatic Speech Recognition (ASR) service using NVIDIA NeMo models with FastAPI, ONNX, and Docker. This project provides an easy-to-use REST API for transcribing speech in WAV audio files using a pre-trained `stt_hi_conformer_ctc_medium` model.

---

## ğŸ“¦ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/vaishu132/nemo-asr-fastapi.git
cd nemo-asr-fastapi
```

### 2. Build the Docker Image

```bash
docker build -t nemo-asr-api .
```

### 3. Run the Container

```bash
docker run -p 8000:8000 nemo-asr-api
```

The API will be accessible at `http://127.0.0.1:8000`.

---

## ğŸš€ Quick Start

Use `curl` or Postman to test the `/transcribe` endpoint:

### ğŸ” Using `curl`:

```bash
curl -X POST "http://127.0.0.1:8000/transcribe" \
  -F "file=@path_to_your_audio.wav"
```

> Ensure your audio file is 16kHz mono WAV and between 5â€“10 seconds long.

---

## ğŸ¯ Design Highlights

### âœ… Model Selection

I have used NVIDIA NeMoâ€™s [stt\_hi\_conformer\_ctc\_medium](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_hi_conformer_ctc_medium) for robust Hindi speech recognition.

### âš¡ ONNX Optimization

The original `.nemo` model is converted to ONNX format for accelerated inference with reduced latency.

## ğŸ“Œ Important Note

please download the .onnx model from https://drive.google.com/file/d/1I2uQq0wHBy-  Jb3qWbKrAMwEF8IyHg5YS/view?usp=sharing and save it as app/model/asr_model.onnx
and then run the main.py

### ğŸ§ª Audio Handling

* Ensures all input audio is:

  * PCM WAV
  * Sampled at 16kHz
  * Between 5â€“10 seconds
* Uses PyDub and NumPy for preprocessing

### ğŸš¦ Endpoint Design

| Method | Endpoint      | Description                    |
| ------ | ------------- | ------------------------------ |
| POST   | `/transcribe` | Upload WAV file and transcribe |

---

## ğŸ› ï¸ Project Structure

```
asr_app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py               # FastAPI app with /transcribe endpoint
â”‚   â”œâ”€â”€ _pycache_
â”‚   â”œâ”€â”€ model/
â”‚       â””â”€â”€vocab.txt
â”‚       â””â”€â”€ stt_hi_conformer.onnx # ONNX exported model (please download the .onnx model from https://drive.google.com/file/d/1I2uQq0wHBy-  Jb3qWbKrAMwEF8IyHg5YS/view?usp=sharing and save it as app/model/asr_model.onnx) 
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ” Future Enhancements

* Batch transcription support
* Streaming inference via WebSocket
* Multilingual model support
* GPU inference via Triton Inference Server

 ForÂ longÂ audios and file formats other than .wav

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

* Open an issue for bugs or feature requests
* Submit a PR for enhancements

---

---

## ğŸ™‹ Questions?

Feel free to open an issue if you need help or have suggestions!

---

